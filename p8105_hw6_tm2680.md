p8105\_hw6\_tm2680
================
Tatini Mal-Sarkar
11/21/2018

Problem 1
=========

``` r
hom_url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

hom_df = 
  read_csv(hom_url) %>% 
  janitor::clean_names() %>% 
  mutate(resolved = as.numeric(disposition == "Closed by arrest"),
         victim_age = as.numeric(victim_age),
         city_state = str_c(city, ", ", state),
         victim_race = ifelse(victim_race != "White", "non-white", "white"),
         victim_race = fct_relevel(victim_race, "white"),
         victim_sex = as.numeric(victim_sex == "Male")) %>% 
  filter(city_state != "Dallas, TX" & city_state != "Phoenix, AZ" & city_state != "Kansas City, MO" & city_state != "Tulsa, AL") %>% 
  select(city_state, resolved, victim_age, victim_race, victim_sex)
```

    ## Parsed with column specification:
    ## cols(
    ##   uid = col_character(),
    ##   reported_date = col_integer(),
    ##   victim_last = col_character(),
    ##   victim_first = col_character(),
    ##   victim_race = col_character(),
    ##   victim_age = col_character(),
    ##   victim_sex = col_character(),
    ##   city = col_character(),
    ##   state = col_character(),
    ##   lat = col_double(),
    ##   lon = col_double(),
    ##   disposition = col_character()
    ## )

    ## Warning in evalq(as.numeric(victim_age), <environment>): NAs introduced by
    ## coercion

``` r
hom_balt_df = 
  hom_df %>% 
  filter(city_state == "Baltimore, MD")

fit_log = 
  hom_balt_df %>% 
  glm(resolved ~ victim_age + victim_sex + victim_race, data = ., family = binomial())

fit_log %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         lower_bound = exp(estimate - 1.96*std.error),
         upper_bound = exp(estimate + 1.96*std.error)) %>% 
  select(term, OR, lower_bound, upper_bound)
```

    ## # A tibble: 4 x 4
    ##   term                    OR lower_bound upper_bound
    ##   <chr>                <dbl>       <dbl>       <dbl>
    ## 1 (Intercept)          3.27        2.07        5.19 
    ## 2 victim_age           0.993       0.987       0.999
    ## 3 victim_sex           0.412       0.315       0.537
    ## 4 victim_racenon-white 0.441       0.313       0.620

The adjusted odds ratio for solving homicides comparing non-white to white victims, keeping all other variables fixed, is 0.4406. We are 95% confident the true value for this OR lies between 0.3129 and 0.6204.

``` r
glm_func = function(df) {
  log = glm(resolved ~ victim_age + victim_sex + victim_race, data = df, family = binomial())
  
  broom::tidy(log) %>% 
    mutate(OR = exp(estimate),
         lower_bound = exp(estimate - 1.96*std.error),
         upper_bound = exp(estimate + 1.96*std.error)) %>% 
    filter(term == "victim_racenon-white") %>% 
    select(OR, lower_bound, upper_bound)
}
```

``` r
hom_df_glm = hom_df %>% 
  nest(resolved, victim_age, victim_sex, victim_race) %>% 
  mutate(models = map(data, glm_func)) %>% 
  unnest(models) %>% 
  select(-data)
```

``` r
hom_df_glm %>% 
  mutate(city_state = forcats::fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_bar(stat = "identity") + 
  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), width = 0.2) + 
  theme(axis.text.x = element_text(angle = 90)) +
  labs(title = "Solving homicides for non-white vs. white victims",
         x = "City",
         y = "OR")
```

![](p8105_hw6_tm2680_files/figure-markdown_github/hom_glm_plot-1.png)

This plot depicts the odds ratio of solving a homicide for a non-white victim compared to a white victim. In cities like Boston, the odds of solving a murder for a non-white victim is 0.118 times the odds of solving a murder for a white victim. We're 95% confident the true value for this OR would lie between 0.049 and 0.285. Because this range doesn't include the null OR value of 1, resolving a homicide and victim race are significantly associated. In other words, in a city like Boston, white victims are likelier to have their case solved. The reverse might hold true in a city like Tampa, where the estimated OR is 1.159. However, because the 95% confidence interval ranges from 0.587 to 2.288, race and resolving a homicide might not be associated.

Problem 2
=========

``` r
bw_df = read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(babysex = as.factor(babysex),
         frace = as.factor(frace),
         mrace = as.factor(mrace),
         missing = sum(is.na(babysex:wtgain)))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_integer(),
    ##   gaweeks = col_double(),
    ##   ppbmi = col_double(),
    ##   smoken = col_double()
    ## )

    ## See spec(...) for full column specifications.

    ## Warning in babysex:wtgain: numerical expression has 4342 elements: only the
    ## first used

    ## Warning in babysex:wtgain: numerical expression has 4342 elements: only the
    ## first used

To build this model, I first narrowed my focus to maternal factors that might influence child birthweight, including pre-pregnancy weight (`ppwt`) and maternal height (`mheight`). I hypothesized that as maternal height and weight increased, so too would the child's weight.

First, I tried linear models including each predictor variable individually.

``` r
# Fit of mother's height alone
fit_mheight = lm(bwt ~ mheight, data = bw_df)
summary(fit_mheight)
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ mheight, data = bw_df)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -2472.14  -285.70    21.86   327.67  1694.86 
    ## 
    ## Coefficients:
    ##             Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  762.089    182.511   4.176 3.03e-05 ***
    ## mheight       37.048      2.872  12.900  < 2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 502.7 on 4340 degrees of freedom
    ## Multiple R-squared:  0.03693,    Adjusted R-squared:  0.0367 
    ## F-statistic: 166.4 on 1 and 4340 DF,  p-value: < 2.2e-16

``` r
# Fit of mother's weight alone
fit_ppwt = lm(bwt ~ ppwt, data = bw_df)
summary(fit_ppwt)
```

    ## 
    ## Call:
    ## lm(formula = bwt ~ ppwt, data = bw_df)
    ## 
    ## Residuals:
    ##      Min       1Q   Median       3Q      Max 
    ## -2521.77  -288.76    21.06   322.49  1748.56 
    ## 
    ## Coefficients:
    ##              Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept) 2540.6691    47.4346   53.56   <2e-16 ***
    ## ppwt           4.6460     0.3791   12.26   <2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 503.6 on 4340 degrees of freedom
    ## Multiple R-squared:  0.03345,    Adjusted R-squared:  0.03323 
    ## F-statistic: 150.2 on 1 and 4340 DF,  p-value: < 2.2e-16

Both seemed important, so I tested whether adding maternal race to family income as predictors made a difference, using ANOVA.

``` r
fit_null = lm(bwt ~ ppwt, data = bw_df)
fit_alt = lm(bwt ~ ppwt + mheight, data = bw_df)

anova(fit_null, fit_alt) %>% 
  broom::tidy() %>% 
  knitr::kable(digit = 3)
```

    ## Warning: Unknown or uninitialised column: 'term'.

|  res.df|         rss|   df|     sumsq|  statistic|  p.value|
|-------:|-----------:|----:|---------:|----------:|--------:|
|    4340|  1100564881|   NA|        NA|         NA|       NA|
|    4339|  1082433270|    1|  18131610|     72.682|        0|

``` r
bw_df %>% 
  lm(bwt ~ ppwt + mheight, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

| term        |  estimate|  std.error|  statistic|  p.value|
|:------------|---------:|----------:|----------:|--------:|
| (Intercept) |  1019.432|    184.535|      5.524|        0|
| ppwt        |     3.134|      0.416|      7.537|        0|
| mheight     |    26.901|      3.155|      8.525|        0|

The combination seemed useful. Then, I wanted to see if maternal smoking modified the effect of maternal height and pre-pregnancy weight.

``` r
bw_df %>% 
  lm(bwt ~ ppwt * smoken + mheight * smoken, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

| term           |  estimate|  std.error|  statistic|  p.value|
|:---------------|---------:|----------:|----------:|--------:|
| (Intercept)    |  1140.416|    208.911|      5.459|    0.000|
| ppwt           |     3.220|      0.491|      6.556|    0.000|
| smoken         |   -48.269|     26.211|     -1.842|    0.066|
| mheight        |    25.232|      3.595|      7.018|    0.000|
| ppwt:smoken    |    -0.013|      0.054|     -0.238|    0.812|
| smoken:mheight |     0.684|      0.445|      1.538|    0.124|

Smoking didn't seem to add anything to our model. Finally, I wanted to test if the interaction of `ppwt` and `mheight` would make a difference.

``` r
bw_df %>% 
  lm(bwt ~ ppwt + mheight + ppwt*mheight, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

| term         |  estimate|  std.error|  statistic|  p.value|
|:-------------|---------:|----------:|----------:|--------:|
| (Intercept)  |    15.175|   1012.960|      0.015|    0.988|
| ppwt         |    11.437|      8.245|      1.387|    0.166|
| mheight      |    42.612|     15.899|      2.680|    0.007|
| ppwt:mheight |    -0.130|      0.128|     -1.008|    0.313|

However, the interaction term was not significant at an alpha of 0.05, so my final model included `ppwt` and `mheight`. As shown above, both predictors achieve significance in a linear model at an alpha of 0.05.

``` r
bw_fit = lm(bwt ~ ppwt + mheight, data = bw_df) 

bw_df %>% 
  modelr::add_predictions(bw_fit) %>% 
  modelr::add_residuals(bw_fit) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  labs(title = "Plot of residuals vs. predicted values",
       x = "Predicted value",
       y = "Residual")
```

![](p8105_hw6_tm2680_files/figure-markdown_github/bw_model-1.png)

``` r
cv_df = crossv_mc(bw_df, 100)

cv_df = cv_df %>% 
  mutate(bw_fit_orig = map(train, ~lm(bwt ~ ppwt + mheight, data = .x)),
         compare_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         compare_2 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(rmse_bw = map2_dbl(bw_fit_orig, test, ~rmse(model = .x, data = .y)),
         rmse_1 = map2_dbl(compare_1, test, ~rmse(model = .x, data = .y)),
         rmse_2 = map2_dbl(compare_2, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() + 
  labs(title = "RMSE by model")
```

![](p8105_hw6_tm2680_files/figure-markdown_github/bw_model_compare-1.png)

The best model appears to be the model that includes head circumference, length, sex, and all interactions, as it has the lowest RMSE value. The RMSE value for my proposed model has the highest RMSE by far, indicating that it is not at all an optimal model.
